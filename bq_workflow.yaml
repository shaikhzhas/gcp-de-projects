# This workflow demonstrates how to use the BigQuery connector.
# The workflow creates a new dataset and then issues a simple query to insert a table with
# some data from bigquery-public-data.usa_names.usa_1910_2013.
# The new table and dataset are both deleted in the following steps.
# Expected successful output: "SUCCESS"

- init:
    assign:
      - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
      - dataset_id: "sdp"
      - table_id: "example_table"
      - query: "SELECT * FROM `bigquery-public-data.usa_names.usa_1910_2013` LIMIT 5000;"
      - create_disposition: "CREATE_IF_NEEDED"  # create a new one if table doesn't exist
      - write_disposition: "WRITE_TRUNCATE"  # truncate it if the table already exists
- create_dataset:
    call: googleapis.bigquery.v2.datasets.insert
    args:
      projectId: ${project_id}
      body:
        datasetReference:
          datasetId: ${dataset_id}
          projectId: ${project_id}
        access[].role: "roles/bigquery.dataViewer"
        access[].specialGroup: "projectReaders"
- insert_table_into_dataset:
    call: googleapis.bigquery.v2.jobs.insert
    args:
      projectId: ${project_id}
      body:
        configuration:
          query:
            query: ${query}
            destinationTable:
              projectId: ${project_id}
              datasetId: ${dataset_id}
              tableId: ${table_id}
            create_disposition: ${create_disposition}
            write_disposition: ${write_disposition}
            allowLargeResults: true
            useLegacySql: false
- the_end:
    return: "SUCCESS"